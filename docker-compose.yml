version: '3.9'

services:
  spark:
    build:
      context: .
      dockerfile: ./Spark3/Dockerfile.dev
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
       - ./Spark3/apps:/opt/spark-apps
       - ./Spark3/data:/opt/spark-data
  spark-worker:
    build:
      context: .
      dockerfile: ./Spark3/DockerfileW.dev
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark
    environment:
      - SPARK_MASTER=spark://spark:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=256M
      - SPARK_DRIVER_MEMORY=256M
      - SPARK_EXECUTOR_MEMORY=256M
    volumes:
       - ./apps:/opt/spark-apps
       - ./data:/opt/spark-data

  app:
    build:
      context: .
      dockerfile: ./hive-metastore/Dockerfile
    environment:
      - DATABASE_HOST=${DATABASE_HOST}
      - DATABASE_DB=${POSTGRES_DB}
      - DATABASE_USER=${POSTGRES_USER}
      - DATABASE_PASSWORD=${POSTGRES_PASSWORD}
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - S3_ENDPOINT_URL=http://localstack:${LOCALSTACK_PORT}
      - S3_BUCKET
      - S3_PREFIX
    ports:
      - '9083:9083'

  test:
    image: python:3.10-alpine
    depends_on:
      app:
        condition: service_healthy
    command: ["./test.sh"]
    profiles:
      - dev
    working_dir: /scripts
    volumes:
      - ./scripts:/scripts
    environment:
      - HIVE_HOST=app
      - HIVE_PORT=9083
